"""
Service ƒë·ªÉ g·ªçi Gemini LLM API
"""
import os
import time
import logging
import hashlib
from typing import Optional, List, Dict, Tuple
import google.generativeai as genai
from config import Config

logger = logging.getLogger(__name__)


def _log_usage(response, label: str = "LLM") -> None:
    """Log s·ªë token input/output/total cho m·ªói l·∫ßn g·ªçi (h·ªó tr·ª£ c·∫£ SDK c≈© v√† m·ªõi)."""
    usage = getattr(response, "usage_metadata", None)
    if usage is None:
        return
    prompt_tokens = getattr(usage, "prompt_token_count", None)
    cached_content_prompt_tokens = getattr(usage, "cached_content_token_count", None)
    output_tokens = getattr(usage, "candidates_token_count", None) or getattr(usage, "output_token_count", None)
    total = getattr(usage, "total_token_count", None)
    if prompt_tokens is not None or output_tokens is not None or total is not None:
        logger.info(
            "[%s] Token usage | input=%s | cached=%s |output=%s | total=%s",
            label,
            prompt_tokens if prompt_tokens is not None else "?",
            cached_content_prompt_tokens if cached_content_prompt_tokens is not None else "?",
            output_tokens if output_tokens is not None else "?",
            total if total is not None else "?",
        )


# SDK m·ªõi (google-genai) d√πng cho prompt caching
try:
    from google import genai as genai_new
    from google.genai import types as genai_types
    _GENAI_NEW_AVAILABLE = True
except ImportError:
    _GENAI_NEW_AVAILABLE = False


class GeminiService:
    """Service ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi Gemini LLM"""

    CACHE_TTL_SECONDS = 3600  # TTL cache m·∫∑c ƒë·ªãnh (1 gi·ªù)

    def __init__(self):
        """Kh·ªüi t·∫°o Gemini client"""
        api_key = Config.GEMINI_API_KEY
        if not api_key:
            raise ValueError("GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong environment variables")

        genai.configure(api_key=api_key)
        model_name = os.getenv('GEMINI_MODEL', 'gemini-2.5-flash')
        self.model = genai.GenerativeModel(model_name)
        self._model_name = model_name
        self._model_name_for_cache = f"models/{model_name}" if not model_name.startswith("models/") else model_name

        self._genai_client: Optional[object] = None
        self._chat_cache: Dict[str, Tuple[str, float]] = {}  # cache_key -> (cache_name, expire_time)

        if _GENAI_NEW_AVAILABLE:
            try:
                self._genai_client = genai_new.Client(api_key=api_key)
                logger.info("ƒê√£ b·∫≠t prompt caching (instruction + product context) v·ªõi google-genai")
            except Exception as e:
                logger.warning("Kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c client caching: %s. Chat s·∫Ω kh√¥ng d√πng cache.", e)
        else:
            logger.info("Ch∆∞a c√†i google-genai; chat kh√¥ng d√πng prompt caching. C√†i: pip install google-genai")

        logger.info(f"ƒê√£ kh·ªüi t·∫°o Gemini client v·ªõi model: {model_name}")
    
    # def classify_intent(
    #     self,
    #     message: str,
    #     conversations: List[Dict],
    #     available_intents: List[Dict]
    # ) -> Dict:
    #     """
    #     Ph√¢n lo·∫°i intent t·ª´ message c·ªßa kh√°ch h√†ng
    #
    #     Args:
    #         message: Tin nh·∫Øn hi·ªán t·∫°i
    #         conversations: L·ªãch s·ª≠ tr√≤ chuy·ªán
    #         available_intents: Danh s√°ch intent c√≥ s·∫µn v·ªõi type v√† description
    #
    #     Returns:
    #         Dict: {'intent': str, 'confidence': float, 'related_intents': List[str]}
    #     """
    #     try:
    #         # X√¢y d·ª±ng prompt ƒë·ªÉ ph√¢n lo·∫°i intent
    #         intent_descriptions = []
    #         for intent in available_intents:
    #             desc = f"- {intent['type']}: {intent['description'] or 'Kh√¥ng c√≥ m√¥ t·∫£'}"
    #             intent_descriptions.append(desc)
    #
    #         # L·∫•y l·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y (t·ªëi ƒëa 5 tin nh·∫Øn cu·ªëi)
    #         recent_conversations = conversations[-5:] if len(conversations) > 5 else conversations
    #         conversation_history = "\n".join([
    #             f"{msg.get('role', 'user')}: {msg.get('content', '')}"
    #             for msg in recent_conversations
    #         ])
    #
    #         prompt = f"""B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n lo·∫°i intent cho chatbot b√°n h√†ng.
    #
    #                 Danh s√°ch c√°c intent c√≥ s·∫µn:
    #                 {chr(10).join(intent_descriptions)}
    #
    #                 L·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y:
    #                 {conversation_history if conversation_history else 'Ch∆∞a c√≥ l·ªãch s·ª≠'}
    #
    #                 Tin nh·∫Øn hi·ªán t·∫°i c·ªßa kh√°ch h√†ng: "{message}"
    #
    #                 H√£y ph√¢n lo·∫°i intent cho tin nh·∫Øn n√†y. Tr·∫£ v·ªÅ theo format JSON:
    #                 {{
    #                     "intent": "t√™n_intent",
    #                     "confidence": 0.0-1.0,
    #                     "related_intents": ["intent1", "intent2"] (n·∫øu c√≥)
    #                 }}
    #
    #                 Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ text th√™m."""
    #
    #         # ƒêo th·ªùi gian g·ªçi LLM
    #         start_time = time.perf_counter()
    #         response = self.model.generate_content(prompt)
    #         elapsed_time = time.perf_counter() - start_time
    #
    #         # Parse response
    #         response_text = response.text.strip()
    #         # Lo·∫°i b·ªè markdown code block n·∫øu c√≥
    #         if response_text.startswith("```"):
    #             response_text = response_text.split("```")[1]
    #             if response_text.startswith("json"):
    #                 response_text = response_text[4:]
    #             response_text = response_text.strip()
    #
    #         import json
    #         result = json.loads(response_text)
    #
    #         logger.info(
    #             f"[LLM] Ph√¢n lo·∫°i intent: {result.get('intent')} v·ªõi confidence: {result.get('confidence')} "
    #             f"- Th·ªùi gian x·ª≠ l√Ω: {elapsed_time:.3f}s"
    #         )
    #         return result
    #
    #     except Exception as e:
    #         logger.error(f"L·ªói khi ph√¢n lo·∫°i intent: {str(e)}")
    #         # Fallback v·ªÅ intent "others"
    #         return {
    #             'intent': 'others',
    #             'confidence': 0.5,
    #             'related_intents': []
    #         }

    def classify_intent(
            self,
            message: str,
            conversations: List[Dict],
            available_intents: List[Dict]
    ) -> Dict:
        try:
            intent_list = "\n".join(
                intent["type"] for intent in available_intents
            )

            recent_conversations = conversations[-2:] if conversations else []
            conversation_text = "\n".join(
                f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                for msg in recent_conversations
            )

            prompt = f"""
                    You are an intent classifier.
                
                    INTENTS:
                    {intent_list}
                
                    Conversation:
                    {conversation_text}
                
                    User message:
                    {message}
                
                    Return ONLY a valid JSON object in this format:
                    {{"intent": "...", "confidence": 0.0-1.0, "related_intents": []}}
                
                    No markdown. No explanation.
                    """

            logger.info("[LLM][Intent] Input g·ª≠i sang Gemini:\n%s", prompt)

            start_time = time.perf_counter()

            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0
                }
            )
            _log_usage(response, "LLM][Intent")

            elapsed = time.perf_counter() - start_time
            text = response.text.strip()

            import json
            result = json.loads(text)

            logger.info(
                f"[LLM][Intent] {result.get('intent')} | "
                f"time={elapsed:.3f}s"
            )

            return {
                "intent": result.get("intent", "others"),
                "confidence": float(result.get("confidence", 0.5)),
                "related_intents": result.get("related_intents", [])
            }

        except Exception as e:
            logger.error(f"L·ªói classify_intent: {str(e)}")
            return {
                "intent": "others",
                "confidence": 0.5,
                "related_intents": []
            }

    # def generate_response(
    #     self,
    #     message: str,
    #     conversations: List[Dict],
    #     context: str,
    #     intent: str
    # ) -> str:
    #     """
    #     T·∫°o ph·∫£n h·ªìi t·ª´ Gemini d·ª±a tr√™n context
    #
    #     Args:
    #         message: Tin nh·∫Øn hi·ªán t·∫°i
    #         conversations: L·ªãch s·ª≠ tr√≤ chuy·ªán
    #         context: Context ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng
    #         intent: Intent ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i
    #
    #     Returns:
    #         str: Ph·∫£n h·ªìi t·ª´ bot
    #     """
    #     try:
    #         # L·∫•y l·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y
    #         recent_conversations = conversations[-10:] if len(conversations) > 10 else conversations
    #         conversation_history = "\n".join([
    #             f"{msg.get('role', 'user')}: {msg.get('content', '')}"
    #             for msg in recent_conversations
    #         ])
    #
    #         prompt = f"""B·∫°n l√† m·ªôt chatbot b√°n h√†ng th√¢n thi·ªán v√† chuy√™n nghi·ªáp.
    #
    #                 Context v√† th√¥ng tin li√™n quan:
    #                 {context}
    #
    #                 L·ªãch s·ª≠ tr√≤ chuy·ªán:
    #                 {conversation_history if conversation_history else 'ƒê√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n'}
    #
    #                 Tin nh·∫Øn c·ªßa kh√°ch h√†ng: "{message}"
    #
    #                 H√£y tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán v√† h·ªØu √≠ch. N·∫øu kh√¥ng c√≥ th√¥ng tin trong context, h√£y n√≥i r√µ v√† ƒë·ªÅ xu·∫•t c√°ch kh√°c ƒë·ªÉ gi√∫p kh√°ch h√†ng."""
    #
    #         # ƒêo th·ªùi gian g·ªçi LLM
    #         start_time = time.perf_counter()
    #         response = self.model.generate_content(prompt)
    #         elapsed_time = time.perf_counter() - start_time
    #
    #         logger.info(
    #             f"[LLM] T·∫°o ph·∫£n h·ªìi cho intent '{intent}' - Th·ªùi gian x·ª≠ l√Ω: {elapsed_time:.3f}s"
    #         )
    #         return response.text.strip()
    #
    #     except Exception as e:
    #         logger.error(f"L·ªói khi t·∫°o ph·∫£n h·ªìi: {str(e)}")
    #         return "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau."

    def generate_response(
            self,
            message: str,
            conversations: List[Dict],
            context: str,
            intent: str
    ) -> str:
        try:
            # ===== 1. Ch·ªâ l·∫•y 2‚Äì3 turn g·∫ßn nh·∫•t =====
            recent_conversations = conversations[-3:] if conversations else []
            conversation_history = "\n".join(
                f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                for msg in recent_conversations
            )

            # ===== 2. Prompt NG·∫ÆN + √âP TI·∫æNG VI·ªÜT =====
            prompt = f"""
            SYSTEM ROLE:
            B·∫°n l√† nh√¢n vi√™n b√°n h√†ng c·ªßa shop.
            NHI·ªÜM V·ª§: tr·∫£ l·ªùi ƒë√∫ng theo intent b√™n d∆∞·ªõi.
            CH·ªà d√πng ti·∫øng Vi·ªát.
            KH√îNG gi·∫£i th√≠ch lan man.
            KH√îNG n√≥i nh·ªØng g√¨ kh√¥ng c√≥ trong context.

            INTENT:
            {intent}

            TH√îNG TIN SHOP / S·∫¢N PH·∫®M:
            {context}

            H·ªòI THO·∫†I G·∫¶N NH·∫§T:
            {conversation_history}

            KH√ÅCH H√ÄNG H·ªéI:
            {message}

            Y√äU C·∫¶U:
            - Tr·∫£ l·ªùi ƒë√∫ng intent
            - Ng·∫Øn g·ªçn
            - L·ªãch s·ª±
            """

            logger.info("[LLM][Generate] Input g·ª≠i sang Gemini:\n%s", prompt)

            start_time = time.perf_counter()

            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0
                }
            )
            _log_usage(response, "LLM][Generate")

            elapsed_time = time.perf_counter() - start_time
            reply = response.text.strip()

            logger.info(
                f"[LLM] Generate response | intent={intent} | time={elapsed_time:.3f}s"
            )

            # ===== 3. Guard: ƒë·∫£m b·∫£o ti·∫øng Vi·ªát =====
            vietnamese_chars = "ƒÉ√¢ƒë√™√¥∆°∆∞√°√†·∫£√£·∫°√©√®·∫ª·∫Ω·∫π√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√∫√π·ªß≈©·ª•√Ω·ª≥·ª∑·ªπ·ªµ"
            if not any(c in reply.lower() for c in vietnamese_chars):
                reply = (
                    "D·∫° b·∫°n ch·ªù shop m·ªôt ch√∫t nh√©, "
                    "m√¨nh s·∫Ω h·ªó tr·ª£ b·∫°n ngay ·∫° üòä"
                )

            return reply

        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o ph·∫£n h·ªìi: {str(e)}")
            return (
                "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. "
                "B·∫°n vui l√≤ng li√™n h·ªá s·ªë 0985006914 ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ nhanh h∆°n nh√© ·∫°."
            )

    def _get_or_create_chat_cache(self, instruction: str, product_context: str) -> Optional[str]:
        """
        L·∫•y ho·∫∑c t·∫°o cache cho instruction + product_context (prompt caching).
        Tr·∫£ v·ªÅ cache.name ƒë·ªÉ d√πng v·ªõi GenerateContentConfig(cached_content=...).
        """
        if not self._genai_client or not _GENAI_NEW_AVAILABLE:
            return None
        cache_key = hashlib.sha256((instruction or "").encode() + (product_context or "").encode()).hexdigest()
        now = time.time()
        if cache_key in self._chat_cache:
            cache_name, expire_time = self._chat_cache[cache_key]
            if now < expire_time:
                logger.info("[LLM][Cache] ƒêang d√πng cache c√≥ s·∫µn (instruction + product context), key=%s...", cache_key[:16])
                return cache_name
            # Cache h·∫øt h·∫°n, x√≥a ƒë·ªÉ t·∫°o m·ªõi
            del self._chat_cache[cache_key]
        # T·∫°o cache m·ªõi
        cached_text = ""
        if instruction:
            cached_text += f"INSTRUCTION (H∆∞·ªõng d·∫´n cho chatbot):\n{instruction}\n\n"
        if product_context:
            cached_text += f"CONTEXT S·∫¢N PH·∫®M:\n{product_context}\n\n"
        if not cached_text.strip():
            return None
        try:
            cache = self._genai_client.caches.create(
                model=self._model_name_for_cache,
                config=genai_types.CreateCachedContentConfig(
                    system_instruction=cached_text.strip(),
                    ttl=f"{self.CACHE_TTL_SECONDS}s",
                ),
            )
            expire_time = now + self.CACHE_TTL_SECONDS
            self._chat_cache[cache_key] = (cache.name, expire_time)
            logger.info("[LLM][Cache] ƒê√£ t·∫°o cache m·ªõi cho instruction + product context (TTL=%ss), key=%s...", self.CACHE_TTL_SECONDS, cache_key[:16])
            return cache.name
        except Exception as e:
            logger.warning("Kh√¥ng t·∫°o ƒë∆∞·ª£c cache: %s. G·ª≠i full prompt.", e)
            return None

    def generate_chat_response(
        self,
        message: str,
        conversations: List[Dict],
        instruction: str = "",
        product_context: str = ""
    ) -> str:
        """
        T·∫°o ph·∫£n h·ªìi chat v·ªõi instruction t√πy ch·ªânh, product context v√† l·ªãch s·ª≠ chat.
        D√πng prompt caching (instruction + product_context) khi c√≥ google-genai.
        """
        try:
            recent_conversations = conversations[-6:] if len(conversations) > 6 else conversations
            conversation_history = ""
            if recent_conversations:
                conversation_history = "\n".join([
                    f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                    for msg in recent_conversations
                ])
            else:
                conversation_history = "ƒê√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc tr√≤ chuy·ªán."

            # Th·ª≠ d√πng prompt caching (instruction + product_context ƒë√£ cache)
            cache_name = self._get_or_create_chat_cache(instruction, product_context)
            if cache_name and self._genai_client and _GENAI_NEW_AVAILABLE:
                logger.info("[LLM] ƒêang d√πng prompt cache (instruction + product context) ‚Äî ch·ªâ g·ª≠i l·ªãch s·ª≠ + tin nh·∫Øn")
                dynamic_prompt = (
                    f"L·ªäCH S·ª¨ TR√í CHUY·ªÜN (6 tin g·∫ßn nh·∫•t):\n{conversation_history}\n\n"
                    f"TIN NH·∫ÆN HI·ªÜN T·∫†I C·ª¶A NG∆Ø·ªúI D√ôNG: {message}\n\n"
                    "H√£y tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán v√† h·ªØu √≠ch d·ª±a tr√™n instruction, context s·∫£n ph·∫©m v√† l·ªãch s·ª≠ tr√≤ chuy·ªán."
                )
                logger.info("[LLM][Chat] Input g·ª≠i sang Gemini (d√πng cache):\n%s", dynamic_prompt)
                start_time = time.perf_counter()
                response = self._genai_client.models.generate_content(
                    model=self._model_name_for_cache,
                    contents=dynamic_prompt,
                    config=genai_types.GenerateContentConfig(
                        cached_content=cache_name,
                        temperature=0.1,
                    ),
                )
                _log_usage(response, "LLM][Chat-cache")
                elapsed_time = time.perf_counter() - start_time
                reply = (response.text or "").strip()
                logger.info("[LLM] Generate chat response (ƒë√£ d√πng cache) - Th·ªùi gian x·ª≠ l√Ω: %.3fs", elapsed_time)
                return reply

            # Fallback: kh√¥ng cache, g·ª≠i full prompt (SDK c≈©)
            logger.info("[LLM] Ch·∫°y b√¨nh th∆∞·ªùng (kh√¥ng d√πng cache) ‚Äî g·ª≠i full prompt (instruction + product context + l·ªãch s·ª≠ + tin nh·∫Øn)")
            prompt_parts = []
            if instruction:
                prompt_parts.append(f"INSTRUCTION (H∆∞·ªõng d·∫´n cho chatbot):\n{instruction}\n")
            if product_context:
                prompt_parts.append(f"CONTEXT S·∫¢N PH·∫®M:\n{product_context}\n")
            prompt_parts.append(f"L·ªäCH S·ª¨ TR√í CHUY·ªÜN (6 tin g·∫ßn nh·∫•t):\n{conversation_history}\n")
            prompt_parts.append(f"TIN NH·∫ÆN HI·ªÜN T·∫†I C·ª¶A NG∆Ø·ªúI D√ôNG: {message}\n")
            prompt_parts.append("H√£y tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán v√† h·ªØu √≠ch d·ª±a tr√™n instruction, context s·∫£n ph·∫©m v√† l·ªãch s·ª≠ tr√≤ chuy·ªán.")
            prompt = "\n".join(prompt_parts)
            logger.info("[LLM][Chat] Input g·ª≠i sang Gemini:\n%s", prompt)
            start_time = time.perf_counter()
            response = self.model.generate_content(
                prompt,
                generation_config={"temperature": 0.7},
            )
            _log_usage(response, "LLM][Chat")
            elapsed_time = time.perf_counter() - start_time
            reply = response.text.strip()
            logger.info("[LLM] Generate chat response (b√¨nh th∆∞·ªùng, kh√¥ng cache) - Th·ªùi gian x·ª≠ l√Ω: %.3fs", elapsed_time)
            return reply

        except Exception as e:
            logger.error("L·ªói khi t·∫°o ph·∫£n h·ªìi chat: %s", e)
            return "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau."


